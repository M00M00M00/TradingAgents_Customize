import time
import json


def create_risk_manager(llm, memory):
    def risk_manager_node(state) -> dict:

        company_name = state["company_of_interest"]

        history = state["risk_debate_state"]["history"]
        risk_debate_state = state["risk_debate_state"]
        market_research_report = state["market_report"]
        news_report = state["news_report"]
        fundamentals_report = state["fundamentals_report"]
        sentiment_report = state["sentiment_report"]
        trader_plan = state["investment_plan"]

        curr_situation = f"{market_research_report}\n\n{sentiment_report}\n\n{news_report}\n\n{fundamentals_report}"
        past_memories = memory.get_memories(curr_situation, n_matches=2)

        past_memory_str = ""
        for i, rec in enumerate(past_memories, 1):
            past_memory_str += rec["recommendation"] + "\n\n"

        prompt = f"""As the Risk Management Judge, evaluate the risk debate and produce a clear recommendation: LONG, SHORT, or NEUTRAL. Avoid NEUTRAL unless confluence is truly low.

Guidelines:
1) Summarize the strongest points from Risky/Neutral/Safe analysts.
2) Rationale: why this decision controls downside (keep risk ~0.5-1% of equity).
3) Refine trader plan **{trader_plan}**: adjust SL within +/-50% of input, size accordingly; set TP for RR 1-10 (prefer 1.5-2.5 unless tight stop justifies higher).
4) Learn from past mistakes: {past_memory_str}

Deliverables: concise LONG/SHORT/NEUTRAL choice + adjustments to SL/size/TP with reasoning.

Analysts Debate History:
{history}"""

        response = llm.invoke(prompt)

        new_risk_debate_state = {
            "judge_decision": response.content,
            "history": risk_debate_state["history"],
            "risky_history": risk_debate_state["risky_history"],
            "safe_history": risk_debate_state["safe_history"],
            "neutral_history": risk_debate_state["neutral_history"],
            "latest_speaker": "Judge",
            "current_risky_response": risk_debate_state["current_risky_response"],
            "current_safe_response": risk_debate_state["current_safe_response"],
            "current_neutral_response": risk_debate_state["current_neutral_response"],
            "count": risk_debate_state["count"],
        }

        return {
            "risk_debate_state": new_risk_debate_state,
            "final_trade_decision": response.content,
        }

    return risk_manager_node
